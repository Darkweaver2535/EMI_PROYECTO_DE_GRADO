"""
Tests para el Analizador de Sentimientos BETO
Sistema OSINT EMI - Sprint 3

Coverage objetivo: ‚â•85%
"""

import pytest
import numpy as np
from unittest.mock import Mock, patch, MagicMock
from datetime import datetime

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))


class TestSentimentAnalyzerInit:
    """Tests para la inicializaci√≥n del SentimentAnalyzer."""
    
    def test_init_default_model(self):
        """Test inicializaci√≥n con modelo por defecto."""
        with patch('ai.sentiment_analyzer.AutoModelForSequenceClassification') as mock_model, \
             patch('ai.sentiment_analyzer.AutoTokenizer') as mock_tokenizer:
            
            mock_tokenizer.from_pretrained.return_value = Mock()
            mock_model.from_pretrained.return_value = Mock()
            
            from ai.sentiment_analyzer import SentimentAnalyzer
            analyzer = SentimentAnalyzer()
            
            assert analyzer.model_name == 'dccuchile/bert-base-spanish-wwm-uncased'
            assert analyzer.num_labels == 3
            assert analyzer.labels == ['Negativo', 'Neutral', 'Positivo']
    
    def test_init_custom_model(self):
        """Test inicializaci√≥n con modelo personalizado."""
        with patch('ai.sentiment_analyzer.AutoModelForSequenceClassification') as mock_model, \
             patch('ai.sentiment_analyzer.AutoTokenizer') as mock_tokenizer:
            
            mock_tokenizer.from_pretrained.return_value = Mock()
            mock_model.from_pretrained.return_value = Mock()
            
            from ai.sentiment_analyzer import SentimentAnalyzer
            analyzer = SentimentAnalyzer(model_name='custom/model', max_length=256)
            
            assert analyzer.model_name == 'custom/model'
            assert analyzer.max_length == 256
    
    def test_label_mapping(self):
        """Test que el mapeo de etiquetas es correcto."""
        with patch('ai.sentiment_analyzer.AutoModelForSequenceClassification') as mock_model, \
             patch('ai.sentiment_analyzer.AutoTokenizer') as mock_tokenizer:
            
            mock_tokenizer.from_pretrained.return_value = Mock()
            mock_model.from_pretrained.return_value = Mock()
            
            from ai.sentiment_analyzer import SentimentAnalyzer
            analyzer = SentimentAnalyzer()
            
            assert analyzer.label2id == {'Negativo': 0, 'Neutral': 1, 'Positivo': 2}
            assert analyzer.id2label == {0: 'Negativo', 1: 'Neutral', 2: 'Positivo'}


class TestSentimentPrediction:
    """Tests para predicci√≥n de sentimientos."""
    
    @pytest.fixture
    def mock_analyzer(self):
        """Fixture para crear un analyzer mockeado."""
        with patch('ai.sentiment_analyzer.AutoModelForSequenceClassification') as mock_model, \
             patch('ai.sentiment_analyzer.AutoTokenizer') as mock_tokenizer, \
             patch('ai.sentiment_analyzer.torch') as mock_torch:
            
            # Mock tokenizer
            tokenizer_instance = Mock()
            tokenizer_instance.return_value = {
                'input_ids': Mock(to=Mock(return_value=Mock())),
                'attention_mask': Mock(to=Mock(return_value=Mock()))
            }
            mock_tokenizer.from_pretrained.return_value = tokenizer_instance
            
            # Mock model
            model_instance = Mock()
            model_instance.to.return_value = model_instance
            model_instance.eval.return_value = model_instance
            
            # Mock output
            mock_logits = Mock()
            mock_logits.logits = Mock()
            model_instance.return_value = mock_logits
            
            mock_model.from_pretrained.return_value = model_instance
            
            # Mock torch
            mock_torch.no_grad.return_value.__enter__ = Mock(return_value=None)
            mock_torch.no_grad.return_value.__exit__ = Mock(return_value=None)
            mock_torch.softmax.return_value.cpu.return_value.numpy.return_value = np.array([[0.1, 0.2, 0.7]])
            mock_torch.argmax.return_value.item.return_value = 2
            
            from ai.sentiment_analyzer import SentimentAnalyzer
            analyzer = SentimentAnalyzer()
            
            yield analyzer
    
    def test_predict_single_text_positive(self, mock_analyzer):
        """Test predicci√≥n de un texto positivo."""
        with patch.object(mock_analyzer, 'predict') as mock_predict:
            mock_predict.return_value = {
                'texto': 'Excelente servicio',
                'sentimiento': 'Positivo',
                'confianza': 0.85,
                'probabilidades': {'Negativo': 0.05, 'Neutral': 0.10, 'Positivo': 0.85}
            }
            
            result = mock_analyzer.predict('Excelente servicio')
            
            assert result['sentimiento'] == 'Positivo'
            assert result['confianza'] >= 0.5
    
    def test_predict_single_text_negative(self, mock_analyzer):
        """Test predicci√≥n de un texto negativo."""
        with patch.object(mock_analyzer, 'predict') as mock_predict:
            mock_predict.return_value = {
                'texto': 'P√©simo servicio',
                'sentimiento': 'Negativo',
                'confianza': 0.90,
                'probabilidades': {'Negativo': 0.90, 'Neutral': 0.07, 'Positivo': 0.03}
            }
            
            result = mock_analyzer.predict('P√©simo servicio')
            
            assert result['sentimiento'] == 'Negativo'
            assert result['confianza'] >= 0.5
    
    def test_predict_batch(self, mock_analyzer):
        """Test predicci√≥n en batch."""
        with patch.object(mock_analyzer, 'predict_batch') as mock_batch:
            mock_batch.return_value = [
                {'texto': 'Texto 1', 'sentimiento': 'Positivo', 'confianza': 0.8},
                {'texto': 'Texto 2', 'sentimiento': 'Negativo', 'confianza': 0.75}
            ]
            
            texts = ['Texto 1', 'Texto 2']
            results = mock_analyzer.predict_batch(texts)
            
            assert len(results) == 2
            assert all('sentimiento' in r for r in results)
    
    def test_predict_empty_text(self, mock_analyzer):
        """Test predicci√≥n con texto vac√≠o."""
        with patch.object(mock_analyzer, 'predict') as mock_predict:
            mock_predict.return_value = {
                'texto': '',
                'sentimiento': 'Neutral',
                'confianza': 0.0,
                'error': 'Texto vac√≠o'
            }
            
            result = mock_analyzer.predict('')
            
            assert 'error' in result or result['sentimiento'] == 'Neutral'


class TestSentimentDataset:
    """Tests para el dataset de sentimientos."""
    
    def test_dataset_creation(self):
        """Test creaci√≥n de dataset."""
        with patch('ai.sentiment_analyzer.torch'):
            from ai.sentiment_analyzer import SentimentDataset
            
            texts = ['Texto 1', 'Texto 2', 'Texto 3']
            labels = [0, 1, 2]
            
            tokenizer = Mock()
            tokenizer.return_value = {
                'input_ids': [1, 2, 3],
                'attention_mask': [1, 1, 1]
            }
            
            dataset = SentimentDataset(texts, labels, tokenizer, max_length=128)
            
            assert len(dataset) == 3
    
    def test_dataset_getitem(self):
        """Test obtenci√≥n de items del dataset."""
        with patch('ai.sentiment_analyzer.torch') as mock_torch:
            mock_torch.tensor.return_value = Mock()
            
            from ai.sentiment_analyzer import SentimentDataset
            
            texts = ['Texto de prueba']
            labels = [1]
            
            tokenizer = Mock()
            tokenizer.return_value = {
                'input_ids': [1, 2, 3],
                'attention_mask': [1, 1, 1]
            }
            
            dataset = SentimentDataset(texts, labels, tokenizer, max_length=128)
            item = dataset[0]
            
            assert 'input_ids' in item
            assert 'attention_mask' in item
            assert 'labels' in item


class TestSentimentEvaluation:
    """Tests para evaluaci√≥n del modelo."""
    
    def test_evaluate_with_metrics(self):
        """Test evaluaci√≥n con m√©tricas."""
        with patch('ai.sentiment_analyzer.AutoModelForSequenceClassification'), \
             patch('ai.sentiment_analyzer.AutoTokenizer'):
            
            from ai.sentiment_analyzer import SentimentAnalyzer
            analyzer = SentimentAnalyzer()
            
            # Mock del m√©todo evaluate
            with patch.object(analyzer, 'evaluate') as mock_eval:
                mock_eval.return_value = {
                    'accuracy': 0.87,
                    'f1_macro': 0.85,
                    'precision': 0.86,
                    'recall': 0.84,
                    'confusion_matrix': [[10, 1, 0], [1, 15, 2], [0, 1, 12]]
                }
                
                results = analyzer.evaluate([], [])
                
                assert results['accuracy'] >= 0.85
                assert 'f1_macro' in results
    
    def test_calculate_metrics(self):
        """Test c√°lculo de m√©tricas espec√≠ficas."""
        y_true = [0, 0, 1, 1, 2, 2]
        y_pred = [0, 0, 1, 0, 2, 2]
        
        # Calcular accuracy manualmente
        correct = sum(1 for t, p in zip(y_true, y_pred) if t == p)
        accuracy = correct / len(y_true)
        
        assert accuracy >= 0.8


class TestModelPersistence:
    """Tests para guardar y cargar modelos."""
    
    def test_save_model(self, tmp_path):
        """Test guardar modelo."""
        with patch('ai.sentiment_analyzer.AutoModelForSequenceClassification') as mock_model, \
             patch('ai.sentiment_analyzer.AutoTokenizer') as mock_tokenizer:
            
            model_instance = Mock()
            model_instance.save_pretrained = Mock()
            tokenizer_instance = Mock()
            tokenizer_instance.save_pretrained = Mock()
            
            mock_model.from_pretrained.return_value = model_instance
            mock_tokenizer.from_pretrained.return_value = tokenizer_instance
            
            from ai.sentiment_analyzer import SentimentAnalyzer
            analyzer = SentimentAnalyzer()
            
            save_path = tmp_path / 'model'
            analyzer.save_model(str(save_path))
            
            model_instance.save_pretrained.assert_called_once()
            tokenizer_instance.save_pretrained.assert_called_once()
    
    def test_load_model(self, tmp_path):
        """Test cargar modelo guardado."""
        with patch('ai.sentiment_analyzer.AutoModelForSequenceClassification') as mock_model, \
             patch('ai.sentiment_analyzer.AutoTokenizer') as mock_tokenizer, \
             patch('os.path.exists', return_value=True):
            
            mock_tokenizer.from_pretrained.return_value = Mock()
            mock_model.from_pretrained.return_value = Mock()
            
            from ai.sentiment_analyzer import SentimentAnalyzer
            analyzer = SentimentAnalyzer()
            
            # El modelo ya se carga en __init__
            assert analyzer.model is not None
            assert analyzer.tokenizer is not None


class TestTextPreprocessing:
    """Tests para preprocesamiento de texto."""
    
    def test_preprocess_basic(self):
        """Test preprocesamiento b√°sico."""
        with patch('ai.sentiment_analyzer.AutoModelForSequenceClassification'), \
             patch('ai.sentiment_analyzer.AutoTokenizer'):
            
            from ai.sentiment_analyzer import SentimentAnalyzer
            analyzer = SentimentAnalyzer()
            
            with patch.object(analyzer, '_preprocess_text') as mock_preprocess:
                mock_preprocess.return_value = 'texto limpio'
                
                result = analyzer._preprocess_text('  Texto LIMPIO  ')
                
                assert isinstance(result, str)
    
    def test_preprocess_special_chars(self):
        """Test preprocesamiento de caracteres especiales."""
        text = "¬°Hola! ¬øC√≥mo est√°s? @usuario #hashtag"
        
        # Limpieza b√°sica
        cleaned = text.replace('@usuario', '').replace('#hashtag', '').strip()
        
        assert '@' not in cleaned or '@usuario' not in cleaned


class TestConfidenceThresholds:
    """Tests para umbrales de confianza."""
    
    def test_high_confidence_prediction(self):
        """Test predicci√≥n con alta confianza."""
        probabilities = np.array([0.05, 0.05, 0.90])
        confidence = np.max(probabilities)
        
        assert confidence >= 0.8
    
    def test_low_confidence_prediction(self):
        """Test predicci√≥n con baja confianza."""
        probabilities = np.array([0.35, 0.35, 0.30])
        confidence = np.max(probabilities)
        
        assert confidence < 0.5
    
    def test_confidence_threshold_filtering(self):
        """Test filtrado por umbral de confianza."""
        predictions = [
            {'sentimiento': 'Positivo', 'confianza': 0.9},
            {'sentimiento': 'Negativo', 'confianza': 0.4},
            {'sentimiento': 'Neutral', 'confianza': 0.7}
        ]
        
        threshold = 0.5
        filtered = [p for p in predictions if p['confianza'] >= threshold]
        
        assert len(filtered) == 2


class TestEdgeCases:
    """Tests para casos extremos."""
    
    def test_very_long_text(self):
        """Test con texto muy largo."""
        long_text = "palabra " * 1000  # Texto muy largo
        
        # Deber√≠a truncarse a max_length tokens
        assert len(long_text.split()) >= 512
    
    def test_unicode_text(self):
        """Test con caracteres unicode."""
        unicode_text = "Texto con emojis üòÄ y caracteres especiales √°√©√≠√≥√∫ √±"
        
        # No deber√≠a fallar
        assert len(unicode_text) > 0
    
    def test_empty_batch(self):
        """Test con batch vac√≠o."""
        empty_batch = []
        
        # Deber√≠a retornar lista vac√≠a
        assert len(empty_batch) == 0
    
    def test_single_word(self):
        """Test con una sola palabra."""
        single_word = "Excelente"
        
        assert len(single_word.split()) == 1


class TestIntegrationMocked:
    """Tests de integraci√≥n con mocks."""
    
    def test_full_pipeline_mocked(self):
        """Test del pipeline completo mockeado."""
        with patch('ai.sentiment_analyzer.AutoModelForSequenceClassification') as mock_model, \
             patch('ai.sentiment_analyzer.AutoTokenizer') as mock_tokenizer, \
             patch('ai.sentiment_analyzer.torch') as mock_torch:
            
            # Setup mocks
            tokenizer_instance = Mock()
            tokenizer_instance.return_value = {
                'input_ids': Mock(to=Mock(return_value=Mock())),
                'attention_mask': Mock(to=Mock(return_value=Mock()))
            }
            mock_tokenizer.from_pretrained.return_value = tokenizer_instance
            
            model_instance = Mock()
            model_instance.to.return_value = model_instance
            model_instance.eval.return_value = model_instance
            mock_model.from_pretrained.return_value = model_instance
            
            from ai.sentiment_analyzer import SentimentAnalyzer
            analyzer = SentimentAnalyzer()
            
            # Verificar que se cre√≥ correctamente
            assert analyzer is not None
            assert analyzer.model is not None
            assert analyzer.tokenizer is not None


if __name__ == '__main__':
    pytest.main([__file__, '-v', '--cov=ai.sentiment_analyzer', '--cov-report=term-missing'])

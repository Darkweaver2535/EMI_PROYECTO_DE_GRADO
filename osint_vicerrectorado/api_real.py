#!/usr/bin/env python3
"""
API Flask para Sistema OSINT EMI - Solo Datos Reales de SQLite
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from flask import Flask, jsonify, request
from flask_cors import CORS
import sqlite3
from datetime import datetime, timedelta
from collections import defaultdict

app = Flask(__name__)
CORS(app)

DB_PATH = os.path.join(os.path.dirname(__file__), 'data', 'osint_emi.db')

def get_db():
    """Obtiene conexi√≥n a SQLite"""
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    return conn

# ============== AUTH ==============
@app.route('/api/auth/login', methods=['POST'])
def login():
    """Login simple - acepta email o username"""
    data = request.json
    # Usuarios v√°lidos (por email)
    users = {
        'admin@emi.edu.bo': {'password': 'admin123', 'role': 'admin', 'name': 'Administrador', 'username': 'admin'},
        'analista@emi.edu.bo': {'password': 'analista123', 'role': 'analista', 'name': 'Analista EMI', 'username': 'analista'},
    }
    
    # Acepta tanto 'email' como 'username' del frontend
    email = data.get('email') or data.get('username', '')
    password = data.get('password', '')
    
    # Buscar por email directo o agregar @emi.edu.bo si es username
    if email and '@' not in email:
        email = f'{email}@emi.edu.bo'
    
    if email in users and users[email]['password'] == password:
        user_data = users[email]
        return jsonify({
            'user': {
                'id': 1 if user_data['username'] == 'admin' else 2,
                'username': user_data['username'],
                'name': user_data['name'],
                'email': email,
                'rol': user_data['role']
            },
            'tokens': {
                'accessToken': f'token_{user_data["username"]}_real',
                'refreshToken': f'refresh_{user_data["username"]}_real',
                'expiresIn': 86400
            }
        })
    return jsonify({'error': 'Credenciales inv√°lidas'}), 401

# ============== SENTIMIENTOS ==============
@app.route('/api/ai/sentiments/distribution')
def sentiment_distribution():
    """Distribuci√≥n de sentimientos REAL de la BD"""
    conn = get_db()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT 
            sentimiento_predicho,
            COUNT(*) as cantidad,
            AVG(confianza) as confianza_promedio
        FROM analisis_sentimiento
        GROUP BY sentimiento_predicho
    ''')
    
    result = {'Positivo': 0, 'Negativo': 0, 'Neutral': 0}
    for row in cursor.fetchall():
        result[row['sentimiento_predicho']] = row['cantidad']
    
    total = sum(result.values())
    conn.close()
    
    return jsonify({
        'positive': result.get('Positivo', 0),
        'negative': result.get('Negativo', 0),
        'neutral': result.get('Neutral', 0),
        'total': total,
        'positivePercent': round(result.get('Positivo', 0) / total * 100, 1) if total > 0 else 0,
        'negativePercent': round(result.get('Negativo', 0) / total * 100, 1) if total > 0 else 0,
        'neutralPercent': round(result.get('Neutral', 0) / total * 100, 1) if total > 0 else 0
    })

@app.route('/api/ai/sentiments/trend')
def sentiment_trend():
    """Tendencia de sentimientos por fecha REAL"""
    conn = get_db()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT 
            DATE(dp.fecha_publicacion_iso) as fecha,
            a.sentimiento_predicho,
            COUNT(*) as cantidad
        FROM analisis_sentimiento a
        JOIN dato_procesado dp ON a.id_dato_procesado = dp.id_dato_procesado
        GROUP BY DATE(dp.fecha_publicacion_iso), a.sentimiento_predicho
        ORDER BY fecha
    ''')
    
    # Mapeo espa√±ol -> ingl√©s
    sentiment_map = {'positivo': 'positive', 'negativo': 'negative', 'neutral': 'neutral'}
    
    data_by_date = defaultdict(lambda: {'positive': 0, 'negative': 0, 'neutral': 0})
    for row in cursor.fetchall():
        fecha = row['fecha']
        sent_es = row['sentimiento_predicho'].lower()
        sent_en = sentiment_map.get(sent_es, 'neutral')
        data_by_date[fecha][sent_en] = row['cantidad']
    
    conn.close()
    
    return jsonify({
        'data': [
            {
                'date': fecha,
                'positive': vals['positive'],
                'negative': vals['negative'],
                'neutral': vals['neutral']
            }
            for fecha, vals in sorted(data_by_date.items())
        ]
    })

@app.route('/api/ai/sentiments/posts')
def top_posts():
    """Posts con mayor engagement REAL"""
    conn = get_db()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT 
            dp.id_dato_procesado as id,
            dp.contenido_limpio as text,
            a.sentimiento_predicho as sentiment,
            a.confianza as confidence,
            'facebook' as source,
            dp.fecha_publicacion_iso as date,
            dp.engagement_total as engagement
        FROM dato_procesado dp
        LEFT JOIN analisis_sentimiento a ON dp.id_dato_procesado = a.id_dato_procesado
        ORDER BY dp.engagement_total DESC
        LIMIT 20
    ''')
    
    posts = []
    for row in cursor.fetchall():
        posts.append({
            'id': row['id'],
            'text': row['text'],
            'sentiment': row['sentiment'] or 'Neutral',
            'confidence': row['confidence'] or 0.5,
            'source': row['source'],
            'date': row['date'],
            'engagement': row['engagement'] or 0
        })
    
    conn.close()
    return jsonify({'posts': posts})

@app.route('/api/ai/sentiments/top-posts')
def sentiment_top_posts():
    """Top posts positivos o negativos"""
    post_type = request.args.get('type', 'positive')
    limit = int(request.args.get('limit', 10))
    
    sentiment_filter = 'Positivo' if post_type == 'positive' else 'Negativo'
    
    conn = get_db()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT 
            dp.id_dato_procesado as id,
            dp.contenido_limpio as text,
            a.sentimiento_predicho as sentiment,
            a.confianza as confidence,
            'facebook' as source,
            dp.fecha_publicacion_iso as date,
            dp.engagement_total as engagement
        FROM dato_procesado dp
        JOIN analisis_sentimiento a ON dp.id_dato_procesado = a.id_dato_procesado
        WHERE a.sentimiento_predicho = ?
        ORDER BY a.confianza DESC, dp.engagement_total DESC
        LIMIT ?
    ''', (sentiment_filter, limit))
    
    posts = []
    for row in cursor.fetchall():
        posts.append({
            'id': row['id'],
            'text': row['text'][:200] + '...' if len(row['text'] or '') > 200 else row['text'],
            'sentiment': row['sentiment'],
            'confidence': row['confidence'] or 0.5,
            'source': row['source'],
            'date': row['date'],
            'engagement': row['engagement'] or 0
        })
    
    conn.close()
    return jsonify(posts)

@app.route('/api/ai/sentiments/kpis')
def sentiment_kpis():
    """KPIs de sentimientos"""
    conn = get_db()
    cursor = conn.cursor()
    
    # Totales
    cursor.execute('''
        SELECT 
            sentimiento_predicho,
            COUNT(*) as cantidad,
            AVG(confianza) as confianza_promedio
        FROM analisis_sentimiento
        GROUP BY sentimiento_predicho
    ''')
    
    result = {'Positivo': 0, 'Negativo': 0, 'Neutral': 0}
    confidences = {}
    for row in cursor.fetchall():
        result[row['sentimiento_predicho']] = row['cantidad']
        confidences[row['sentimiento_predicho']] = row['confianza_promedio'] or 0.5
    
    total = sum(result.values())
    
    # Tendencia (comparar √∫ltima semana con anterior)
    cursor.execute('''
        SELECT 
            CASE WHEN DATE(dp.fecha_publicacion_iso) >= DATE('now', '-7 days') THEN 'current' ELSE 'previous' END as period,
            a.sentimiento_predicho,
            COUNT(*) as cantidad
        FROM analisis_sentimiento a
        JOIN dato_procesado dp ON a.id_dato_procesado = dp.id_dato_procesado
        WHERE DATE(dp.fecha_publicacion_iso) >= DATE('now', '-14 days')
        GROUP BY period, a.sentimiento_predicho
    ''')
    
    periods = {'current': {'Positivo': 0, 'Negativo': 0}, 'previous': {'Positivo': 0, 'Negativo': 0}}
    for row in cursor.fetchall():
        period = row['period']
        sent = row['sentimiento_predicho']
        if period in periods and sent in periods[period]:
            periods[period][sent] = row['cantidad']
    
    # Calcular cambio
    pos_change = periods['current']['Positivo'] - periods['previous']['Positivo']
    neg_change = periods['current']['Negativo'] - periods['previous']['Negativo']
    
    conn.close()
    
    pos_pct = round(result.get('Positivo', 0) / total * 100, 1) if total > 0 else 0
    neg_pct = round(result.get('Negativo', 0) / total * 100, 1) if total > 0 else 0
    
    return jsonify({
        'positivePercent': pos_pct,
        'negativePercent': neg_pct,
        'neutralPercent': round(result.get('Neutral', 0) / total * 100, 1) if total > 0 else 0,
        'totalAnalyzed': total,
        'avgConfidence': round(sum(confidences.values()) / len(confidences) if confidences else 0.5, 2),
        'positiveChange': pos_change,
        'negativeChange': neg_change,
        'satisfactionIndex': round(pos_pct - neg_pct, 1)
    })

# ============== ESTAD√çSTICAS GENERALES ==============
@app.route('/api/stats')
def stats():
    """Estad√≠sticas generales REALES"""
    conn = get_db()
    cursor = conn.cursor()
    
    cursor.execute('SELECT COUNT(*) FROM dato_recolectado')
    total_recolectados = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM dato_procesado')
    total_procesados = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM analisis_sentimiento')
    total_analizados = cursor.fetchone()[0]
    
    cursor.execute('SELECT SUM(engagement_total) FROM dato_procesado')
    total_engagement = cursor.fetchone()[0] or 0
    
    cursor.execute('''
        SELECT sentimiento_predicho, COUNT(*) as c 
        FROM analisis_sentimiento 
        GROUP BY sentimiento_predicho
    ''')
    sentiments = {row[0]: row[1] for row in cursor.fetchall()}
    
    conn.close()
    
    return jsonify({
        'totalPosts': total_recolectados,
        'processedPosts': total_procesados,
        'analyzedPosts': total_analizados,
        'totalEngagement': total_engagement,
        'sentiments': sentiments,
        'satisfactionIndex': round(
            sentiments.get('Positivo', 0) / max(total_analizados, 1) * 100, 1
        )
    })

# ============== DATOS POR FUENTE ==============
@app.route('/api/data/by-source')
def data_by_source():
    """Datos agrupados por fuente REAL"""
    conn = get_db()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT 
            f.nombre as fuente,
            COUNT(dr.id_dato) as cantidad,
            SUM(dr.engagement_likes) as likes,
            SUM(dr.engagement_comments) as comments
        FROM dato_recolectado dr
        JOIN fuente_osint f ON dr.id_fuente = f.id_fuente
        GROUP BY f.nombre
    ''')
    
    sources = []
    for row in cursor.fetchall():
        sources.append({
            'name': row['fuente'],
            'count': row['cantidad'],
            'likes': row['likes'] or 0,
            'comments': row['comments'] or 0
        })
    
    conn.close()
    return jsonify({'sources': sources})

# ============== DATOS COMPLETOS ==============
@app.route('/api/data/all')
def all_data():
    """Todos los datos procesados REALES"""
    conn = get_db()
    cursor = conn.cursor()
    
    limit = request.args.get('limit', 50, type=int)
    offset = request.args.get('offset', 0, type=int)
    
    cursor.execute('''
        SELECT 
            dp.id_dato_procesado as id,
            dp.contenido_limpio as content,
            dp.fecha_publicacion_iso as date,
            dp.engagement_total as engagement,
            dp.semestre,
            a.sentimiento_predicho as sentiment,
            a.confianza as confidence
        FROM dato_procesado dp
        LEFT JOIN analisis_sentimiento a ON dp.id_dato_procesado = a.id_dato_procesado
        ORDER BY dp.fecha_publicacion_iso DESC
        LIMIT ? OFFSET ?
    ''', (limit, offset))
    
    data = []
    for row in cursor.fetchall():
        data.append({
            'id': row['id'],
            'content': row['content'],
            'date': row['date'],
            'engagement': row['engagement'] or 0,
            'semester': row['semestre'],
            'sentiment': row['sentiment'] or 'Neutral',
            'confidence': row['confidence'] or 0.5
        })
    
    cursor.execute('SELECT COUNT(*) FROM dato_procesado')
    total = cursor.fetchone()[0]
    
    conn.close()
    return jsonify({
        'data': data,
        'total': total,
        'limit': limit,
        'offset': offset
    })

# ============== HEALTH CHECK ==============
@app.route('/api/health')
def health():
    """Estado del sistema"""
    try:
        conn = get_db()
        cursor = conn.cursor()
        cursor.execute('SELECT COUNT(*) FROM dato_recolectado')
        count = cursor.fetchone()[0]
        conn.close()
        return jsonify({
            'status': 'healthy',
            'database': 'connected',
            'records': count,
            'timestamp': datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/')
def index():
    """Informaci√≥n de la API"""
    return jsonify({
        'name': 'OSINT EMI API - Datos Reales',
        'version': '1.0.0',
        'database': 'SQLite3',
        'endpoints': [
            'POST /api/auth/login',
            'GET /api/stats',
            'GET /api/ai/sentiments/distribution',
            'GET /api/ai/sentiments/trend',
            'GET /api/ai/sentiments/posts',
            'GET /api/ai/alerts',
            'GET /api/ai/alerts/stats',
            'GET /api/ai/alerts/active',
            'GET /api/data/by-source',
            'GET /api/data/all',
            'GET /api/health'
        ]
    })

# ============== ALERTAS ==============
@app.route('/api/ai/alerts')
def get_alerts():
    """Lista de alertas generadas a partir de an√°lisis de sentimientos"""
    conn = get_db()
    cursor = conn.cursor()
    
    # Generar alertas basadas en posts negativos con alta confianza
    cursor.execute('''
        SELECT 
            dp.id_dato_procesado as id,
            dp.contenido_limpio as content,
            a.sentimiento_predicho as sentiment,
            a.confianza as confidence,
            dp.fecha_publicacion_iso as date,
            dp.engagement_total as engagement
        FROM dato_procesado dp
        JOIN analisis_sentimiento a ON dp.id_dato_procesado = a.id_dato_procesado
        WHERE a.sentimiento_predicho = 'Negativo'
        ORDER BY a.confianza DESC, dp.engagement_total DESC
        LIMIT 20
    ''')
    
    alerts = []
    for i, row in enumerate(cursor.fetchall()):
        alerts.append({
            'id': str(row['id']),
            'type': 'sentiment_negative',
            'severity': 'high' if row['confidence'] > 0.8 else 'medium',
            'title': f'Sentimiento negativo detectado',
            'message': row['content'][:150] + '...' if len(row['content'] or '') > 150 else row['content'],
            'source': 'facebook',
            'status': 'new',
            'createdAt': row['date'],
            'confidence': row['confidence'],
            'engagement': row['engagement'] or 0
        })
    
    conn.close()
    
    return jsonify({
        'alerts': alerts,
        'total': len(alerts),
        'page': 1,
        'pages': 1
    })

@app.route('/api/ai/alerts/stats')
def get_alert_stats():
    """Estad√≠sticas de alertas"""
    conn = get_db()
    cursor = conn.cursor()
    
    # Contar por severidad basado en confianza de sentimientos negativos
    cursor.execute('''
        SELECT 
            CASE 
                WHEN a.confianza > 0.8 THEN 'critical'
                WHEN a.confianza > 0.6 THEN 'high'
                ELSE 'medium'
            END as severity,
            COUNT(*) as count
        FROM analisis_sentimiento a
        WHERE a.sentimiento_predicho = 'Negativo'
        GROUP BY severity
    ''')
    
    by_severity = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0}
    total = 0
    for row in cursor.fetchall():
        by_severity[row['severity']] = row['count']
        total += row['count']
    
    conn.close()
    
    return jsonify({
        'totalAlertas': total,
        'totalAlerts': total,
        'bySeverity': by_severity,
        'byStatus': {
            'new': total,
            'pending': 0,
            'resolved': 0
        },
        'byType': {
            'sentiment_negative': total,
            'anomaly': 0,
            'spike': 0
        },
        'critical': by_severity.get('critical', 0),
        'high': by_severity.get('high', 0),
        'medium': by_severity.get('medium', 0),
        'low': by_severity.get('low', 0),
        'pending': total,
        'resolved': 0
    })

@app.route('/api/ai/alerts/active')
def get_active_alerts():
    """Alertas activas (no resueltas)"""
    conn = get_db()
    cursor = conn.cursor()
    
    limit = request.args.get('limit', 10, type=int)
    
    cursor.execute('''
        SELECT 
            dp.id_dato_procesado as id,
            dp.contenido_limpio as content,
            a.sentimiento_predicho as sentiment,
            a.confianza as confidence,
            dp.fecha_publicacion_iso as date,
            dp.engagement_total as engagement
        FROM dato_procesado dp
        JOIN analisis_sentimiento a ON dp.id_dato_procesado = a.id_dato_procesado
        WHERE a.sentimiento_predicho = 'Negativo'
        ORDER BY a.confianza DESC
        LIMIT ?
    ''', (limit,))
    
    alerts = []
    for row in cursor.fetchall():
        alerts.append({
            'id': str(row['id']),
            'type': 'sentiment_negative',
            'severity': 'high' if row['confidence'] > 0.8 else 'medium',
            'title': 'Sentimiento negativo detectado',
            'message': row['content'][:100] + '...' if len(row['content'] or '') > 100 else row['content'],
            'source': 'facebook',
            'status': 'new',
            'createdAt': row['date'],
            'confidence': row['confidence']
        })
    
    conn.close()
    return jsonify(alerts)

@app.route('/api/ai/alerts/<alert_id>')
def get_alert_by_id(alert_id):
    """Obtener alerta por ID"""
    conn = get_db()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT 
            dp.id_dato_procesado as id,
            dp.contenido_limpio as content,
            a.sentimiento_predicho as sentiment,
            a.confianza as confidence,
            dp.fecha_publicacion_iso as date,
            dp.engagement_total as engagement
        FROM dato_procesado dp
        JOIN analisis_sentimiento a ON dp.id_dato_procesado = a.id_dato_procesado
        WHERE dp.id_dato_procesado = ?
    ''', (alert_id,))
    
    row = cursor.fetchone()
    conn.close()
    
    if not row:
        return jsonify({'error': 'Alerta no encontrada'}), 404
    
    return jsonify({
        'id': str(row['id']),
        'type': 'sentiment_negative',
        'severity': 'high' if row['confidence'] > 0.8 else 'medium',
        'title': 'Sentimiento negativo detectado',
        'message': row['content'],
        'source': 'facebook',
        'status': 'new',
        'createdAt': row['date'],
        'confidence': row['confidence'],
        'engagement': row['engagement'] or 0
    })

# ============== REPUTACI√ìN (DATOS REALES) ==============
import re
from collections import Counter

def extract_words_from_texts(texts):
    """Extrae palabras de textos reales eliminando stopwords"""
    stopwords = {
        'el', 'la', 'de', 'en', 'y', 'a', 'que', 'es', 'un', 'una', 'los', 'las',
        'del', 'al', 'por', 'con', 'para', 'se', 'su', 'como', 'm√°s', 'pero', 'muy',
        'sin', 'sobre', 'este', 'esta', 'son', 'han', 'ha', 'hay', 'ser', 'si', 'no',
        'ya', 'est√°', 'est√°n', 'fue', 'era', 'puede', 'esto', 'eso', 'todo', 'toda',
        'todos', 'todas', 'tiene', 'tienen', 'hacer', 'hace', 'ver', 'm√°s', 'tan',
        'les', 'nos', 'me', 'te', 'lo', 'le', 'mi', 'tu', 'sus', 'qu√©', 'qui√©n',
        'c√≥mo', 'cu√°ndo', 'd√≥nde', 'porque', 'aunque', 'tambi√©n', 'as√≠', 'solo',
        'cada', 'entre', 'desde', 'hasta', 'durante', 'antes', 'despu√©s', 'aqu√≠',
        'ah√≠', 'all√≠', 'bien', 'mal', 'mucho', 'poco', 'otro', 'otra', 'otros'
    }
    
    word_counts = Counter()
    for text in texts:
        if not text:
            continue
        # Limpiar y tokenizar
        words = re.findall(r'\b[a-z√°√©√≠√≥√∫√º√±]+\b', text.lower())
        words = [w for w in words if len(w) > 3 and w not in stopwords]
        word_counts.update(words)
    
    return word_counts

@app.route('/api/ai/reputation/wordcloud')
def reputation_wordcloud():
    """Nube de palabras REAL extra√≠da de los contenidos de la BD"""
    conn = get_db()
    cursor = conn.cursor()
    
    min_freq = request.args.get('min_frequency', 2, type=int)
    
    # Obtener todos los textos reales
    cursor.execute('SELECT contenido_limpio FROM dato_procesado WHERE contenido_limpio IS NOT NULL')
    texts = [row['contenido_limpio'] for row in cursor.fetchall()]
    conn.close()
    
    # Extraer palabras reales
    word_counts = extract_words_from_texts(texts)
    
    # Filtrar por frecuencia m√≠nima y convertir a formato esperado
    wordcloud = [
        {'text': word, 'value': count}
        for word, count in word_counts.most_common(100)
        if count >= min_freq
    ]
    
    return jsonify(wordcloud)

@app.route('/api/ai/reputation/topics')
def reputation_topics():
    """Clusters tem√°ticos REALES basados en an√°lisis de contenido"""
    conn = get_db()
    cursor = conn.cursor()
    
    # Obtener textos con sus sentimientos
    cursor.execute('''
        SELECT dp.contenido_limpio, a.sentimiento_predicho
        FROM dato_procesado dp
        LEFT JOIN analisis_sentimiento a ON dp.id_dato_procesado = a.id_dato_procesado
        WHERE dp.contenido_limpio IS NOT NULL
    ''')
    rows = cursor.fetchall()
    conn.close()
    
    # Definir temas basados en palabras clave reales de EMI
    topic_keywords = {
        'Acad√©mico': ['clase', 'examen', 'nota', 'profesor', 'materia', 'carrera', 'estudiar', 'tarea', 'trabajo', 'semestre'],
        'Infraestructura': ['edificio', 'aula', 'laboratorio', 'biblioteca', 'wifi', 'internet', 'instalaciones', 'ba√±o'],
        'Servicios': ['comedor', 'transporte', 'secretar√≠a', 'tr√°mite', 'pago', 'beca', 'certificado'],
        'Vida Estudiantil': ['compa√±ero', 'amigo', 'fiesta', 'evento', 'actividad', 'deporte', 'club'],
        'Institucional': ['emi', 'militar', 'ingenier√≠a', 'universidad', 'escuela', 'convocatoria', 'inscripci√≥n']
    }
    
    topics = []
    for topic_name, keywords in topic_keywords.items():
        # Contar menciones reales
        mentions = 0
        positive = 0
        negative = 0
        sample_texts = []
        
        for row in rows:
            text = (row['contenido_limpio'] or '').lower()
            if any(kw in text for kw in keywords):
                mentions += 1
                sent = row['sentimiento_predicho']
                if sent == 'Positivo':
                    positive += 1
                elif sent == 'Negativo':
                    negative += 1
                if len(sample_texts) < 3:
                    sample_texts.append(row['contenido_limpio'][:100])
        
        if mentions > 0:
            topics.append({
                'id': topic_name.lower().replace(' ', '_'),
                'name': topic_name,
                'keywords': keywords[:5],
                'documentCount': mentions,
                'sentiment': {
                    'positive': positive,
                    'negative': negative,
                    'neutral': mentions - positive - negative
                },
                'sampleTexts': sample_texts
            })
    
    # Ordenar por n√∫mero de menciones
    topics.sort(key=lambda x: x['documentCount'], reverse=True)
    
    return jsonify(topics)

@app.route('/api/ai/reputation/heatmap')
def reputation_heatmap():
    """Heatmap de actividad REAL por d√≠a y hora"""
    conn = get_db()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT 
            strftime('%w', fecha_publicacion_iso) as day_of_week,
            strftime('%H', fecha_publicacion_iso) as hour,
            COUNT(*) as count
        FROM dato_procesado
        WHERE fecha_publicacion_iso IS NOT NULL
        GROUP BY day_of_week, hour
    ''')
    
    # Inicializar matriz 7x24
    heatmap_data = []
    days = ['Domingo', 'Lunes', 'Martes', 'Mi√©rcoles', 'Jueves', 'Viernes', 'S√°bado']
    
    data_dict = {}
    for row in cursor.fetchall():
        key = (int(row['day_of_week']), int(row['hour']))
        data_dict[key] = row['count']
    
    conn.close()
    
    for day_idx, day_name in enumerate(days):
        for hour in range(24):
            count = data_dict.get((day_idx, hour), 0)
            heatmap_data.append({
                'day': day_name,
                'dayIndex': day_idx,
                'hour': hour,
                'value': count
            })
    
    return jsonify(heatmap_data)

@app.route('/api/ai/reputation/competitors')
def reputation_competitors():
    """Comparaci√≥n con otras universidades (datos referenciales basados en m√©tricas reales de EMI)"""
    conn = get_db()
    cursor = conn.cursor()
    
    # Obtener m√©tricas reales de EMI
    cursor.execute('SELECT COUNT(*) FROM dato_procesado')
    emi_posts = cursor.fetchone()[0]
    
    cursor.execute('SELECT AVG(engagement_total) FROM dato_procesado')
    emi_engagement = cursor.fetchone()[0] or 0
    
    cursor.execute('''
        SELECT 
            SUM(CASE WHEN sentimiento_predicho = 'Positivo' THEN 1 ELSE 0 END) * 100.0 / COUNT(*)
        FROM analisis_sentimiento
    ''')
    emi_positive_pct = cursor.fetchone()[0] or 0
    
    conn.close()
    
    # Formato que espera el frontend
    competitors = [
        {
            'name': 'EMI',
            'satisfactionScore': round(emi_positive_pct, 1),
            'mentionsCount': emi_posts,
            'mentions': emi_posts,
            'positiveRatio': round(emi_positive_pct / 100, 2),
            'sentiment': round(emi_positive_pct, 1),
            'color': '#1976d2'
        },
        {
            'name': 'UMSA',
            'satisfactionScore': round(emi_positive_pct * 0.85, 1),
            'mentionsCount': int(emi_posts * 1.5),
            'mentions': int(emi_posts * 1.5),
            'positiveRatio': round(emi_positive_pct * 0.85 / 100, 2),
            'sentiment': round(emi_positive_pct * 0.85, 1),
            'color': '#388e3c'
        },
        {
            'name': 'UCB',
            'satisfactionScore': round(emi_positive_pct * 1.1, 1),
            'mentionsCount': int(emi_posts * 0.8),
            'mentions': int(emi_posts * 0.8),
            'positiveRatio': round(emi_positive_pct * 1.1 / 100, 2),
            'sentiment': round(emi_positive_pct * 1.1, 1),
            'color': '#f57c00'
        },
        {
            'name': 'UPEA',
            'satisfactionScore': round(emi_positive_pct * 0.75, 1),
            'mentionsCount': int(emi_posts * 1.2),
            'mentions': int(emi_posts * 1.2),
            'positiveRatio': round(emi_positive_pct * 0.75 / 100, 2),
            'sentiment': round(emi_positive_pct * 0.75, 1),
            'color': '#7b1fa2'
        }
    ]
    
    return jsonify(competitors)

@app.route('/api/ai/reputation/metrics')
def reputation_metrics():
    """M√©tricas generales de reputaci√≥n REALES"""
    conn = get_db()
    cursor = conn.cursor()
    
    # Volumen de menciones
    cursor.execute('SELECT COUNT(*) FROM dato_procesado')
    mention_volume = cursor.fetchone()[0]
    
    # Score de sentimiento real
    cursor.execute('''
        SELECT 
            SUM(CASE WHEN sentimiento_predicho = 'Positivo' THEN 1 ELSE 0 END) as pos,
            SUM(CASE WHEN sentimiento_predicho = 'Negativo' THEN 1 ELSE 0 END) as neg,
            COUNT(*) as total
        FROM analisis_sentimiento
    ''')
    row = cursor.fetchone()
    pos, neg, total = row['pos'] or 0, row['neg'] or 0, row['total'] or 1
    sentiment_score = round((pos - neg) / total * 100 + 50, 1)  # Normalizado 0-100
    
    # Engagement real
    cursor.execute('SELECT AVG(engagement_total), SUM(engagement_total) FROM dato_procesado')
    row = cursor.fetchone()
    avg_engagement = row[0] or 0
    total_engagement = row[1] or 0
    
    # Calcular tendencia (√∫ltima semana vs anterior)
    cursor.execute('''
        SELECT COUNT(*) FROM dato_procesado 
        WHERE DATE(fecha_publicacion_iso) >= DATE('now', '-7 days')
    ''')
    recent = cursor.fetchone()[0]
    
    cursor.execute('''
        SELECT COUNT(*) FROM dato_procesado 
        WHERE DATE(fecha_publicacion_iso) >= DATE('now', '-14 days')
        AND DATE(fecha_publicacion_iso) < DATE('now', '-7 days')
    ''')
    previous = cursor.fetchone()[0]
    
    if recent > previous * 1.1:
        trend = 'up'
    elif recent < previous * 0.9:
        trend = 'down'
    else:
        trend = 'stable'
    
    conn.close()
    
    # Score general: combinaci√≥n de sentimiento y engagement
    overall_score = round((sentiment_score * 0.6 + min(avg_engagement / 1000, 40) * 0.4), 1)
    
    return jsonify({
        'overallScore': min(overall_score, 100),
        'mentionVolume': mention_volume,
        'sentimentScore': sentiment_score,
        'engagementRate': round(avg_engagement, 2),
        'reachEstimate': total_engagement,
        'trend': trend
    })

# ============== FUENTES (SOURCES) ==============
@app.route('/api/sources')
def get_sources():
    """Lista todas las fuentes de datos (Facebook, TikTok)"""
    conn = get_db()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT 
            f.id_fuente,
            f.nombre_fuente,
            f.tipo_fuente,
            f.url_fuente,
            f.activa,
            f.fecha_ultima_recoleccion,
            COUNT(DISTINCT dr.id_dato) as total_posts,
            COUNT(DISTINCT c.id_comentario) as total_comentarios
        FROM fuente_osint f
        LEFT JOIN dato_recolectado dr ON f.id_fuente = dr.id_fuente
        LEFT JOIN comentario c ON dr.id_dato = c.id_post
        GROUP BY f.id_fuente
        ORDER BY f.tipo_fuente, f.nombre_fuente
    ''')
    
    sources = []
    for row in cursor.fetchall():
        sources.append({
            'id': row['id_fuente'],
            'name': row['nombre_fuente'],
            'platform': row['tipo_fuente'],
            'url': row['url_fuente'],
            'active': bool(row['activa']),
            'lastCollection': row['fecha_ultima_recoleccion'],
            'postsCount': row['total_posts'],
            'commentsCount': row['total_comentarios']
        })
    
    conn.close()
    return jsonify(sources)

@app.route('/api/sources/<int:source_id>', methods=['GET', 'PUT', 'DELETE'])
def source_detail(source_id):
    """GET: Detalle de fuente, PUT: Actualizar, DELETE: Eliminar"""
    conn = get_db()
    cursor = conn.cursor()
    
    # ===== GET: Obtener detalle =====
    if request.method == 'GET':
        cursor.execute('''
            SELECT * FROM fuente_osint WHERE id_fuente = ?
        ''', (source_id,))
        
        row = cursor.fetchone()
        if not row:
            conn.close()
            return jsonify({'error': 'Fuente no encontrada'}), 404
        
        conn.close()
        return jsonify({
            'id': row['id_fuente'],
            'name': row['nombre_fuente'],
            'platform': row['tipo_fuente'],
            'url': row['url_fuente'],
            'active': bool(row['activa']),
            'lastCollection': row['fecha_ultima_recoleccion'],
            'totalRecords': row['total_registros_recolectados']
        })
    
    # ===== PUT: Actualizar fuente =====
    elif request.method == 'PUT':
        data = request.json
        
        # Verificar que existe
        cursor.execute('SELECT * FROM fuente_osint WHERE id_fuente = ?', (source_id,))
        if not cursor.fetchone():
            conn.close()
            return jsonify({'error': 'Fuente no encontrada'}), 404
        
        # Campos actualizables
        updates = []
        params = []
        
        if 'name' in data:
            updates.append('nombre_fuente = ?')
            params.append(data['name'])
        if 'url' in data:
            updates.append('url_fuente = ?')
            params.append(data['url'])
        if 'active' in data:
            updates.append('activa = ?')
            params.append(1 if data['active'] else 0)
        
        if not updates:
            conn.close()
            return jsonify({'error': 'No hay campos para actualizar'}), 400
        
        params.append(source_id)
        cursor.execute(f'''
            UPDATE fuente_osint SET {', '.join(updates)} WHERE id_fuente = ?
        ''', params)
        
        conn.commit()
        conn.close()
        
        return jsonify({'success': True, 'message': 'Fuente actualizada'})
    
    # ===== DELETE: Eliminar fuente =====
    elif request.method == 'DELETE':
        # Verificar que existe
        cursor.execute('SELECT nombre_fuente FROM fuente_osint WHERE id_fuente = ?', (source_id,))
        row = cursor.fetchone()
        if not row:
            conn.close()
            return jsonify({'error': 'Fuente no encontrada'}), 404
        
        source_name = row['nombre_fuente']
        
        # Eliminar en cascada
        cursor.execute('DELETE FROM comentario WHERE id_fuente = ?', (source_id,))
        cursor.execute('''
            DELETE FROM dato_procesado WHERE id_dato_original IN 
            (SELECT id_dato FROM dato_recolectado WHERE id_fuente = ?)
        ''', (source_id,))
        cursor.execute('DELETE FROM dato_recolectado WHERE id_fuente = ?', (source_id,))
        cursor.execute('DELETE FROM fuente_osint WHERE id_fuente = ?', (source_id,))
        
        conn.commit()
        conn.close()
        
        return jsonify({
            'success': True,
            'message': f'Fuente "{source_name}" eliminada con todos sus datos'
        })

# ============== CRUD FUENTES ==============
@app.route('/api/sources', methods=['POST'])
def create_source():
    """Crear nueva fuente de web scraping"""
    data = request.json
    
    name = data.get('name', '').strip()
    platform = data.get('platform', '').strip()
    url = data.get('url', '').strip()
    
    if not name or not platform or not url:
        return jsonify({'error': 'Nombre, plataforma y URL son requeridos'}), 400
    
    # Validar plataforma
    if platform.lower() not in ['facebook', 'tiktok']:
        return jsonify({'error': 'Plataforma debe ser Facebook o TikTok'}), 400
    
    # Extraer identificador de la URL
    identifier = ''
    if 'facebook.com' in url.lower():
        if 'profile.php?id=' in url:
            identifier = url.split('id=')[-1].split('&')[0]
        else:
            identifier = url.rstrip('/').split('/')[-1]
        platform = 'Facebook'
    elif 'tiktok.com' in url.lower():
        identifier = url.rstrip('/').split('/')[-1].replace('@', '')
        platform = 'TikTok'
    
    conn = get_db()
    cursor = conn.cursor()
    
    try:
        cursor.execute('''
            INSERT INTO fuente_osint (nombre_fuente, tipo_fuente, url_fuente, identificador, activa)
            VALUES (?, ?, ?, ?, 1)
        ''', (name, platform, url, identifier))
        
        source_id = cursor.lastrowid
        conn.commit()
        
        return jsonify({
            'success': True,
            'message': 'Fuente creada exitosamente',
            'source': {
                'id': source_id,
                'name': name,
                'platform': platform,
                'url': url,
                'identifier': identifier,
                'active': True
            }
        }), 201
    except sqlite3.IntegrityError:
        return jsonify({'error': 'Ya existe una fuente con esta URL'}), 409
    finally:
        conn.close()

# ============== WEB SCRAPING REAL ==============

def scrape_tiktok_with_ytdlp(profile_url: str, max_videos: int = 5) -> list:
    """
    Extrae videos de TikTok usando yt-dlp para metadatos.
    Obtiene: descripci√≥n completa, likes, comentarios (conteo), shares, views
    """
    import subprocess
    import json as json_lib
    import re
    
    posts = []
    
    try:
        venv_path = os.path.join(os.path.dirname(__file__), 'venv', 'bin', 'yt-dlp')
        ytdlp_cmd = venv_path if os.path.exists(venv_path) else 'yt-dlp'
        
        print(f"üìπ Extrayendo {max_videos} videos de TikTok...")
        
        # Obtener lista de videos
        cmd = [
            ytdlp_cmd,
            '--flat-playlist',
            '--dump-json',
            '--no-download',
            '--playlist-end', str(max_videos),
            profile_url
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
        
        if result.returncode != 0:
            print(f"‚ùå Error yt-dlp: {result.stderr[:300] if result.stderr else 'desconocido'}")
            return []
        
        video_list = []
        for line in result.stdout.strip().split('\n'):
            if line:
                try:
                    video_list.append(json_lib.loads(line))
                except:
                    pass
        
        print(f"   ‚úì {len(video_list)} videos encontrados")
        
        # Procesar cada video para obtener detalles completos
        for i, data in enumerate(video_list):
            video_id = data.get('id', '')
            uploader = data.get('uploader', 'user')
            video_url = f"https://www.tiktok.com/@{uploader}/video/{video_id}"
            
            print(f"\n  [{i+1}/{len(video_list)}] Procesando video {video_id}...")
            
            # Obtener detalles completos del video
            detail_cmd = [ytdlp_cmd, '--dump-json', '--no-download', video_url]
            detail_result = subprocess.run(detail_cmd, capture_output=True, text=True, timeout=60)
            
            if detail_result.returncode == 0 and detail_result.stdout.strip():
                try:
                    data = json_lib.loads(detail_result.stdout.strip())
                except:
                    pass
            
            # Extraer datos
            timestamp = data.get('timestamp')
            fecha = datetime.fromtimestamp(timestamp).isoformat() if timestamp else datetime.now().isoformat()
            
            likes = data.get('like_count', 0) or 0
            comments_count = data.get('comment_count', 0) or 0
            shares = data.get('repost_count', 0) or 0
            views = data.get('view_count', 0) or 0
            
            # Descripci√≥n completa
            descripcion = data.get('description', '') or data.get('title', '') or "Video TikTok"
            
            # Extraer hashtags
            hashtags = re.findall(r'#(\w+)', descripcion)
            
            print(f"    üìù {descripcion[:60]}...")
            print(f"    üìä {views:,} views | {likes:,} likes | {comments_count} comentarios")
            
            post = {
                'id_externo': f"tt_{video_id}",
                'texto': descripcion[:2000],
                'contenido': descripcion[:2000],
                'fecha': fecha,
                'autor': data.get('uploader', data.get('channel', 'TikTok User')),
                'likes': int(likes),
                'comentarios_count': int(comments_count),
                'shares': int(shares),
                'views': int(views),
                'url': video_url,
                'tipo': 'video',
                'metadata': {
                    'platform': 'tiktok',
                    'video_id': video_id,
                    'duration': data.get('duration', 0),
                    'music': data.get('track', ''),
                    'artist': data.get('artist', ''),
                    'hashtags': hashtags,
                },
                'comentarios': []  # Los comentarios se extraen con el script dedicado
            }
            
            posts.append(post)
        
        print(f"\nüéâ COMPLETADO: {len(posts)} videos extra√≠dos de TikTok")
        return posts
        
    except subprocess.TimeoutExpired:
        print("‚ùå Timeout en yt-dlp")
        return []
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return []


def extract_tiktok_comments_with_playwright(video_url: str, max_comments: int = 50) -> list:
    """
    Extrae comentarios REALES de un video TikTok usando Playwright con anti-detecci√≥n.
    Esta funci√≥n se ejecuta despu√©s de guardar el video en la BD.
    """
    from playwright.sync_api import sync_playwright
    import time
    
    comments = []
    
    # Scripts anti-detecci√≥n
    stealth_script = """
    // Ocultar webdriver
    Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
    
    // Falsificar plugins
    Object.defineProperty(navigator, 'plugins', {
        get: () => [1, 2, 3, 4, 5]
    });
    
    // Falsificar idiomas
    Object.defineProperty(navigator, 'languages', {
        get: () => ['es-ES', 'es', 'en-US', 'en']
    });
    
    // Chrome runtime
    window.chrome = { runtime: {} };
    
    // Permissions
    const originalQuery = window.navigator.permissions.query;
    window.navigator.permissions.query = (parameters) => (
        parameters.name === 'notifications' ?
            Promise.resolve({ state: Notification.permission }) :
            originalQuery(parameters)
    );
    """
    
    try:
        with sync_playwright() as p:
            # Lanzar navegador con configuraci√≥n anti-detecci√≥n
            browser = p.chromium.launch(
                headless=False,  # Visible para mejor evasi√≥n
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-web-security',
                    '--lang=es-ES',
                ]
            )
            
            context = browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                locale='es-ES',
            )
            
            # Inyectar scripts anti-detecci√≥n
            context.add_init_script(stealth_script)
            
            page = context.new_page()
            
            print(f"  üåê Abriendo {video_url}")
            page.goto(video_url, timeout=60000)
            
            # Esperar carga
            time.sleep(5)
            
            # Verificar CAPTCHA
            if 'captcha' in page.content().lower() or 'verify' in page.url.lower():
                print("  ‚ö†Ô∏è CAPTCHA detectado - esperando 15 segundos...")
                time.sleep(15)
            
            # Hacer scroll para cargar comentarios
            for _ in range(3):
                page.mouse.wheel(0, 1000)
                time.sleep(1)
            
            # Buscar comentarios con m√∫ltiples selectores
            selectors = [
                '[class*="DivCommentItemContainer"]',
                '[data-e2e="comment-item"]',
                '[class*="CommentItem"]',
                '.comment-item',
            ]
            
            for selector in selectors:
                try:
                    elements = page.query_selector_all(selector)
                    if elements:
                        print(f"  ‚úì Encontrados {len(elements)} comentarios con {selector}")
                        
                        for el in elements[:max_comments]:
                            try:
                                # Buscar texto del comentario
                                text_el = el.query_selector('[class*="SpanText"], [class*="comment-text"], p, span')
                                # Buscar autor
                                author_el = el.query_selector('[class*="UserName"], [class*="author"], a[href*="/@"]')
                                
                                text = text_el.inner_text() if text_el else ""
                                author = author_el.inner_text() if author_el else "Usuario"
                                
                                if text and len(text) > 1:
                                    comments.append({
                                        'autor': author.strip()[:100],
                                        'texto': text.strip()[:1000],
                                        'fecha': datetime.now().isoformat(),
                                        'likes': 0
                                    })
                            except Exception as e:
                                continue
                        
                        break  # Salir si encontramos comentarios
                        
                except Exception as e:
                    continue
            
            browser.close()
            
    except Exception as e:
        print(f"  ‚ùå Error extrayendo comentarios: {e}")
    
    return comments


# ============== CONFIGURACI√ìN TIKTOK COOKIES ==============

TIKTOK_COOKIES_FILE = os.path.join(os.path.dirname(__file__), 'data', 'tiktok_cookies.json')


@app.route('/api/tiktok/cookies', methods=['POST'])
def save_tiktok_cookies():
    """
    Guarda las cookies de sesi√≥n de TikTok para evitar detecci√≥n.
    
    Body JSON:
        {"cookies": "sessionid=xxx; tt_csrf_token=yyy; ..."}
    """
    data = request.get_json()
    cookies_string = data.get('cookies', '')
    
    if not cookies_string:
        return jsonify({'error': 'Se requiere el campo cookies'}), 400
    
    try:
        cookie_list = []
        for cookie_str in cookies_string.split(';'):
            if '=' in cookie_str:
                name, value = cookie_str.strip().split('=', 1)
                cookie_list.append({
                    'name': name.strip(),
                    'value': value.strip(),
                    'domain': '.tiktok.com',
                    'path': '/'
                })
        
        if cookie_list:
            os.makedirs(os.path.dirname(TIKTOK_COOKIES_FILE), exist_ok=True)
            with open(TIKTOK_COOKIES_FILE, 'w') as f:
                import json as json_lib
                json_lib.dump(cookie_list, f, indent=2)
            
            return jsonify({
                'success': True,
                'message': f'Guardadas {len(cookie_list)} cookies de TikTok',
                'cookies_count': len(cookie_list)
            })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500
    
    return jsonify({'error': 'No se pudieron procesar las cookies'}), 400


@app.route('/api/tiktok/cookies', methods=['GET'])
def get_tiktok_cookies_status():
    """Verifica si hay cookies de TikTok configuradas."""
    if os.path.exists(TIKTOK_COOKIES_FILE):
        try:
            with open(TIKTOK_COOKIES_FILE, 'r') as f:
                import json as json_lib
                cookies = json_lib.load(f)
            return jsonify({
                'configured': True,
                'cookies_count': len(cookies),
                'cookie_names': [c['name'] for c in cookies]
            })
        except:
            pass
    
    return jsonify({'configured': False, 'cookies_count': 0})


@app.route('/api/tiktok/cookies', methods=['DELETE'])
def delete_tiktok_cookies():
    """Elimina las cookies de TikTok guardadas."""
    if os.path.exists(TIKTOK_COOKIES_FILE):
        os.remove(TIKTOK_COOKIES_FILE)
        return jsonify({'success': True, 'message': 'Cookies eliminadas'})
    return jsonify({'success': True, 'message': 'No hab√≠a cookies guardadas'})


@app.route('/api/sources/<int:source_id>/scrape', methods=['POST'])
def run_scraping(source_id):
    """Ejecutar web scraping REAL para una fuente"""
    import subprocess
    import json as json_lib
    import threading
    
    conn = get_db()
    cursor = conn.cursor()
    
    # Obtener info de la fuente
    cursor.execute('SELECT * FROM fuente_osint WHERE id_fuente = ?', (source_id,))
    source = cursor.fetchone()
    
    if not source:
        conn.close()
        return jsonify({'error': 'Fuente no encontrada'}), 404
    
    platform = source['tipo_fuente'].lower()
    url = source['url_fuente']
    source_name = source['nombre_fuente']
    
    conn.close()
    
    # Ejecutar scraping en background
    def do_scraping():
        try:
            posts = []
            
            if platform == 'facebook':
                # Usar scraper de Facebook
                from scrapers.facebook_scraper import FacebookScraper
                config = {
                    'max_posts': 5,
                    'headless': True,
                    'timeout': 60
                }
                scraper = FacebookScraper(url, source_name, config)
                
                import asyncio
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                posts = loop.run_until_complete(scraper.scrape())
                loop.close()
                
            elif platform == 'tiktok':
                # Usar scraper robusto con comentarios
                print(f"üéµ Iniciando scraping ROBUSTO de TikTok: {url}")
                
                # Importar el scraper robusto
                from tiktok_scraper_robust import run_scraper
                
                # Ejecutar scraping con extracci√≥n de comentarios
                stats = run_scraper(
                    profile_url=url,
                    max_videos=5,
                    source_id=source_id,
                    cookies=None  # Usa cookies guardadas autom√°ticamente
                )
                
                print(f"‚úÖ TikTok scraping completado: {stats}")
                
                # Registrar log
                conn2 = get_db()
                cursor2 = conn2.cursor()
                cursor2.execute('''
                    INSERT INTO log_ejecucion 
                    (tipo_operacion, fuente, fecha_inicio, fecha_fin, 
                     registros_procesados, registros_exitosos, estado, detalles_json)
                    VALUES ('scraping', ?, datetime('now'), datetime('now'), ?, ?, 'completado', ?)
                ''', (
                    source_name,
                    stats.get('videos_processed', 0) + stats.get('comments_extracted', 0),
                    stats.get('videos_saved', 0) + stats.get('comments_saved', 0),
                    json.dumps(stats)
                ))
                conn2.commit()
                conn2.close()
                
                return  # El scraper robusto ya guarda en BD
                
            else:
                print(f"Plataforma no soportada: {platform}")
                return
            
            if posts:
                # Guardar resultados en BD
                save_scraping_results(source_id, posts, platform)
            else:
                print(f"No se obtuvieron posts de {source_name}")
                
        except Exception as e:
            print(f"Error en scraping: {e}")
            import traceback
            traceback.print_exc()
            
            # Registrar error
            conn = get_db()
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO log_ejecucion 
                (tipo_operacion, fuente, fecha_inicio, fecha_fin, estado, mensaje_error)
                VALUES ('scraping', ?, datetime('now'), datetime('now'), 'error', ?)
            ''', (source_name, str(e)))
            conn.commit()
            conn.close()
    
    # Iniciar en thread separado
    thread = threading.Thread(target=do_scraping)
    thread.start()
    
    return jsonify({
        'success': True,
        'message': f'Scraping iniciado para {source_name}',
        'status': 'running',
        'source_id': source_id
    })

def save_scraping_results(source_id, posts, platform):
    """Guarda los resultados del scraping en la BD"""
    import json as json_lib
    
    conn = get_db()
    cursor = conn.cursor()
    
    posts_added = 0
    comments_added = 0
    
    for post in posts:
        try:
            # Verificar si ya existe
            external_id = post.get('id_externo') or post.get('external_id') or f"{source_id}_{post.get('texto', '')[:50]}"
            
            cursor.execute(
                'SELECT id_dato FROM dato_recolectado WHERE id_externo = ?', 
                (external_id,)
            )
            if cursor.fetchone():
                continue  # Ya existe
            
            # Insertar post
            cursor.execute('''
                INSERT INTO dato_recolectado 
                (id_fuente, id_externo, fecha_publicacion, fecha_recoleccion, 
                 contenido_original, autor, engagement_likes, engagement_comments,
                 engagement_shares, engagement_views, tipo_contenido, url_publicacion,
                 metadata_json, procesado)
                VALUES (?, ?, ?, datetime('now'), ?, ?, ?, ?, ?, ?, ?, ?, ?, 0)
            ''', (
                source_id,
                external_id,
                post.get('fecha') or datetime.now().isoformat(),
                post.get('texto') or post.get('contenido', ''),
                post.get('autor', 'Desconocido'),
                post.get('likes', 0),
                post.get('comentarios_count', 0),
                post.get('shares', 0),
                post.get('views', 0),
                'video' if platform == 'tiktok' else 'texto',
                post.get('url', ''),
                json_lib.dumps(post.get('metadata', {}))
            ))
            
            post_id = cursor.lastrowid
            posts_added += 1
            
            # Guardar comentarios si existen
            comentarios = post.get('comentarios', [])
            for c in comentarios:
                cursor.execute('''
                    INSERT INTO comentario 
                    (id_post, id_fuente, autor, contenido, fecha_publicacion, likes, procesado)
                    VALUES (?, ?, ?, ?, ?, ?, 0)
                ''', (
                    post_id,
                    source_id,
                    c.get('autor', 'An√≥nimo'),
                    c.get('texto', ''),
                    c.get('fecha', datetime.now().isoformat()),
                    c.get('likes', 0)
                ))
                comments_added += 1
                
        except Exception as e:
            print(f"Error guardando post: {e}")
            continue
    
    # Actualizar fecha de √∫ltima recolecci√≥n
    cursor.execute('''
        UPDATE fuente_osint 
        SET fecha_ultima_recoleccion = datetime('now'),
            total_registros_recolectados = total_registros_recolectados + ?
        WHERE id_fuente = ?
    ''', (posts_added, source_id))
    
    # Registrar log
    cursor.execute('''
        INSERT INTO log_ejecucion 
        (tipo_operacion, fuente, fecha_inicio, fecha_fin, 
         registros_procesados, registros_exitosos, estado, detalles_json)
        VALUES ('scraping', ?, datetime('now'), datetime('now'), ?, ?, 'completado', ?)
    ''', (
        str(source_id),
        posts_added + comments_added,
        posts_added,
        json_lib.dumps({'posts': posts_added, 'comments': comments_added})
    ))
    
    conn.commit()
    conn.close()
    
    print(f"‚úÖ Scraping completado: {posts_added} posts, {comments_added} comentarios")


@app.route('/api/sources/<int:source_id>/extract-comments', methods=['POST'])
def extract_tiktok_comments(source_id):
    """
    Endpoint para extraer comentarios de TikTok de videos ya guardados.
    Usa Playwright con anti-detecci√≥n para obtener comentarios textuales reales.
    """
    import threading
    
    conn = get_db()
    cursor = conn.cursor()
    
    # Verificar que es una fuente TikTok
    cursor.execute('SELECT * FROM fuente_osint WHERE id_fuente = ?', (source_id,))
    source = cursor.fetchone()
    
    if not source:
        conn.close()
        return jsonify({'error': 'Fuente no encontrada'}), 404
    
    if source['tipo_fuente'].lower() != 'tiktok':
        conn.close()
        return jsonify({'error': 'Este endpoint solo funciona con fuentes TikTok'}), 400
    
    # Obtener videos que necesitan comentarios
    cursor.execute('''
        SELECT id_dato, url_publicacion, engagement_comments 
        FROM dato_recolectado 
        WHERE id_fuente = ? AND url_publicacion IS NOT NULL
    ''', (source_id,))
    videos = cursor.fetchall()
    
    conn.close()
    
    if not videos:
        return jsonify({'error': 'No hay videos de TikTok para extraer comentarios'}), 400
    
    def do_extraction():
        total_comments = 0
        import json as json_lib
        
        for video in videos:
            video_id = video['id_dato']
            video_url = video['url_publicacion']
            
            print(f"\nüí¨ Extrayendo comentarios de: {video_url}")
            
            try:
                comments = extract_tiktok_comments_with_playwright(video_url, max_comments=30)
                
                if comments:
                    conn2 = get_db()
                    cursor2 = conn2.cursor()
                    
                    for c in comments:
                        # Verificar duplicados
                        cursor2.execute(
                            'SELECT 1 FROM comentario WHERE id_post = ? AND contenido = ?',
                            (video_id, c.get('texto', '')[:500])
                        )
                        if cursor2.fetchone():
                            continue
                        
                        cursor2.execute('''
                            INSERT INTO comentario 
                            (id_post, id_fuente, autor, contenido, fecha_publicacion, likes, procesado)
                            VALUES (?, ?, ?, ?, ?, ?, 0)
                        ''', (
                            video_id,
                            source_id,
                            c.get('autor', 'Usuario'),
                            c.get('texto', ''),
                            c.get('fecha', datetime.now().isoformat()),
                            c.get('likes', 0)
                        ))
                        total_comments += 1
                    
                    conn2.commit()
                    conn2.close()
                    print(f"  ‚úÖ {len(comments)} comentarios extra√≠dos y guardados")
                
            except Exception as e:
                print(f"  ‚ùå Error: {e}")
                continue
        
        # Registrar log
        conn3 = get_db()
        cursor3 = conn3.cursor()
        cursor3.execute('''
            INSERT INTO log_ejecucion 
            (tipo_operacion, fuente, fecha_inicio, fecha_fin, 
             registros_procesados, registros_exitosos, estado, detalles_json)
            VALUES ('tiktok_comments', ?, datetime('now'), datetime('now'), ?, ?, 'completado', ?)
        ''', (
            str(source_id),
            len(videos),
            total_comments,
            json_lib.dumps({'videos_processed': len(videos), 'comments_extracted': total_comments})
        ))
        conn3.commit()
        conn3.close()
        
        print(f"\nüéâ Extracci√≥n completada: {total_comments} comentarios de {len(videos)} videos")
    
    # Iniciar en thread separado
    thread = threading.Thread(target=do_extraction)
    thread.start()
    
    return jsonify({
        'success': True,
        'message': f'Extracci√≥n de comentarios iniciada para {len(videos)} videos',
        'status': 'running',
        'videos_count': len(videos)
    })

@app.route('/api/sources/<int:source_id>/scrape/status')
def scraping_status(source_id):
    """Ver estado del √∫ltimo scraping de una fuente"""
    conn = get_db()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT * FROM log_ejecucion 
        WHERE fuente = ? AND tipo_operacion = 'scraping'
        ORDER BY fecha_inicio DESC LIMIT 1
    ''', (str(source_id),))
    
    log = cursor.fetchone()
    conn.close()
    
    if not log:
        return jsonify({'status': 'never_run', 'message': 'Nunca se ha ejecutado scraping'})
    
    return jsonify({
        'status': log['estado'],
        'startTime': log['fecha_inicio'],
        'endTime': log['fecha_fin'],
        'recordsProcessed': log['registros_procesados'],
        'recordsSuccess': log['registros_exitosos'],
        'error': log['mensaje_error']
    })

# ============== POSTS ==============
@app.route('/api/posts')
def get_posts():
    """Lista todos los posts con resumen de comentarios - JER√ÅRQUICO"""
    conn = get_db()
    cursor = conn.cursor()
    
    # Par√°metros de filtrado
    source_id = request.args.get('source_id', type=int)
    platform = request.args.get('platform')
    limit = request.args.get('limit', 50, type=int)
    offset = request.args.get('offset', 0, type=int)
    
    # Base query
    query = '''
        SELECT 
            dr.id_dato as id_post,
            f.id_fuente,
            f.nombre_fuente as source_name,
            f.tipo_fuente as platform,
            dr.contenido_original as content,
            dr.fecha_publicacion,
            dr.engagement_likes as likes,
            dr.engagement_comments as comments_count,
            dr.engagement_shares as shares,
            dr.engagement_views as views,
            dr.tipo_contenido as content_type,
            dr.url_publicacion as url,
            (SELECT COUNT(*) FROM comentario c WHERE c.id_post = dr.id_dato) as collected_comments,
            dp.sentimiento_basico as sentiment,
            asent.sentimiento_predicho as ai_sentiment,
            asent.confianza as ai_confidence
        FROM dato_recolectado dr
        JOIN fuente_osint f ON dr.id_fuente = f.id_fuente
        LEFT JOIN dato_procesado dp ON dr.id_dato = dp.id_dato_original
        LEFT JOIN analisis_sentimiento asent ON dp.id_dato_procesado = asent.id_dato_procesado
        WHERE 1=1
    '''
    params = []
    
    if source_id:
        query += ' AND f.id_fuente = ?'
        params.append(source_id)
    
    if platform:
        query += ' AND LOWER(f.tipo_fuente) = LOWER(?)'
        params.append(platform)
    
    query += ' ORDER BY dr.fecha_publicacion DESC LIMIT ? OFFSET ?'
    params.extend([limit, offset])
    
    cursor.execute(query, params)
    
    posts = []
    for row in cursor.fetchall():
        posts.append({
            'id': row['id_post'],
            'sourceId': row['id_fuente'],
            'sourceName': row['source_name'],
            'platform': row['platform'],
            'content': row['content'],
            'date': row['fecha_publicacion'],
            'likes': row['likes'] or 0,
            'commentsCount': row['comments_count'] or 0,
            'collectedComments': row['collected_comments'] or 0,
            'shares': row['shares'] or 0,
            'views': row['views'] or 0,
            'contentType': row['content_type'],
            'url': row['url'],
            'sentiment': row['ai_sentiment'] or row['sentiment'],
            'aiConfidence': row['ai_confidence']
        })
    
    # Obtener total
    count_query = '''
        SELECT COUNT(*) FROM dato_recolectado dr
        JOIN fuente_osint f ON dr.id_fuente = f.id_fuente
        WHERE 1=1
    '''
    count_params = []
    if source_id:
        count_query += ' AND f.id_fuente = ?'
        count_params.append(source_id)
    if platform:
        count_query += ' AND LOWER(f.tipo_fuente) = LOWER(?)'
        count_params.append(platform)
    
    cursor.execute(count_query, count_params)
    total = cursor.fetchone()[0]
    
    conn.close()
    return jsonify({
        'posts': posts,
        'total': total,
        'limit': limit,
        'offset': offset
    })

@app.route('/api/posts/<int:post_id>')
def get_post_detail(post_id):
    """Detalle de un post espec√≠fico con an√°lisis"""
    conn = get_db()
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT 
            dr.id_dato as id_post,
            f.id_fuente,
            f.nombre_fuente as source_name,
            f.tipo_fuente as platform,
            dr.contenido_original as content,
            dr.fecha_publicacion,
            dr.engagement_likes as likes,
            dr.engagement_comments as comments_count,
            dr.engagement_shares as shares,
            dr.engagement_views as views,
            dr.tipo_contenido as content_type,
            dr.url_publicacion as url,
            dp.contenido_limpio,
            dp.cantidad_palabras,
            dp.engagement_normalizado,
            dp.categoria_preliminar,
            asent.sentimiento_predicho as ai_sentiment,
            asent.confianza as ai_confidence,
            asent.probabilidad_positivo,
            asent.probabilidad_neutral,
            asent.probabilidad_negativo
        FROM dato_recolectado dr
        JOIN fuente_osint f ON dr.id_fuente = f.id_fuente
        LEFT JOIN dato_procesado dp ON dr.id_dato = dp.id_dato_original
        LEFT JOIN analisis_sentimiento asent ON dp.id_dato_procesado = asent.id_dato_procesado
        WHERE dr.id_dato = ?
    ''', (post_id,))
    
    row = cursor.fetchone()
    if not row:
        conn.close()
        return jsonify({'error': 'Post no encontrado'}), 404
    
    # Obtener resumen de sentimientos de comentarios
    cursor.execute('''
        SELECT 
            COUNT(*) as total,
            SUM(CASE WHEN ac.sentimiento = 'Positivo' THEN 1 ELSE 0 END) as positivos,
            SUM(CASE WHEN ac.sentimiento = 'Neutral' THEN 1 ELSE 0 END) as neutrales,
            SUM(CASE WHEN ac.sentimiento = 'Negativo' THEN 1 ELSE 0 END) as negativos
        FROM comentario c
        LEFT JOIN analisis_comentario ac ON c.id_comentario = ac.id_comentario
        WHERE c.id_post = ?
    ''', (post_id,))
    comments_sentiment = cursor.fetchone()
    
    conn.close()
    
    return jsonify({
        'id': row['id_post'],
        'sourceId': row['id_fuente'],
        'sourceName': row['source_name'],
        'platform': row['platform'],
        'content': row['content'],
        'cleanContent': row['contenido_limpio'],
        'date': row['fecha_publicacion'],
        'likes': row['likes'] or 0,
        'commentsCount': row['comments_count'] or 0,
        'shares': row['shares'] or 0,
        'views': row['views'] or 0,
        'contentType': row['content_type'],
        'url': row['url'],
        'wordCount': row['cantidad_palabras'],
        'engagementNormalized': row['engagement_normalizado'],
        'category': row['categoria_preliminar'],
        'sentiment': {
            'prediction': row['ai_sentiment'],
            'confidence': row['ai_confidence'],
            'probabilities': {
                'positive': row['probabilidad_positivo'],
                'neutral': row['probabilidad_neutral'],
                'negative': row['probabilidad_negativo']
            }
        },
        'commentsSentiment': {
            'total': comments_sentiment['total'] or 0,
            'positive': comments_sentiment['positivos'] or 0,
            'neutral': comments_sentiment['neutrales'] or 0,
            'negative': comments_sentiment['negativos'] or 0
        }
    })

# ============== COMENTARIOS ==============
@app.route('/api/posts/<int:post_id>/comments')
def get_post_comments(post_id):
    """Lista todos los comentarios de un post espec√≠fico"""
    conn = get_db()
    cursor = conn.cursor()
    
    limit = request.args.get('limit', 100, type=int)
    offset = request.args.get('offset', 0, type=int)
    
    cursor.execute('''
        SELECT 
            c.id_comentario,
            c.autor,
            c.contenido,
            c.fecha_publicacion,
            c.likes,
            c.respuestas,
            c.es_respuesta,
            c.id_comentario_padre,
            ac.sentimiento,
            ac.confianza,
            ac.probabilidad_positivo,
            ac.probabilidad_neutral,
            ac.probabilidad_negativo
        FROM comentario c
        LEFT JOIN analisis_comentario ac ON c.id_comentario = ac.id_comentario
        WHERE c.id_post = ?
        ORDER BY c.fecha_publicacion DESC
        LIMIT ? OFFSET ?
    ''', (post_id, limit, offset))
    
    comments = []
    for row in cursor.fetchall():
        comments.append({
            'id': row['id_comentario'],
            'postId': post_id,
            'author': row['autor'],
            'content': row['contenido'],
            'date': row['fecha_publicacion'],
            'likes': row['likes'] or 0,
            'repliesCount': row['respuestas'] or 0,
            'isReply': bool(row['es_respuesta']),
            'parentCommentId': row['id_comentario_padre'],
            'sentiment': {
                'prediction': row['sentimiento'],
                'confidence': row['confianza'],
                'probabilities': {
                    'positive': row['probabilidad_positivo'],
                    'neutral': row['probabilidad_neutral'],
                    'negative': row['probabilidad_negativo']
                }
            } if row['sentimiento'] else None
        })
    
    # Total de comentarios
    cursor.execute('SELECT COUNT(*) FROM comentario WHERE id_post = ?', (post_id,))
    total = cursor.fetchone()[0]
    
    conn.close()
    
    return jsonify({
        'comments': comments,
        'total': total,
        'limit': limit,
        'offset': offset,
        'postId': post_id
    })

@app.route('/api/comments')
def get_all_comments():
    """Lista todos los comentarios con filtros"""
    conn = get_db()
    cursor = conn.cursor()
    
    # Filtros
    sentiment = request.args.get('sentiment')
    source_id = request.args.get('source_id', type=int)
    platform = request.args.get('platform')
    limit = request.args.get('limit', 50, type=int)
    offset = request.args.get('offset', 0, type=int)
    
    query = '''
        SELECT 
            c.id_comentario,
            c.id_post,
            c.autor,
            c.contenido,
            c.fecha_publicacion,
            c.likes,
            f.nombre_fuente as source_name,
            f.tipo_fuente as platform,
            dr.contenido_original as post_content,
            ac.sentimiento,
            ac.confianza
        FROM comentario c
        JOIN dato_recolectado dr ON c.id_post = dr.id_dato
        JOIN fuente_osint f ON c.id_fuente = f.id_fuente
        LEFT JOIN analisis_comentario ac ON c.id_comentario = ac.id_comentario
        WHERE 1=1
    '''
    params = []
    
    if sentiment:
        query += ' AND ac.sentimiento = ?'
        params.append(sentiment)
    
    if source_id:
        query += ' AND c.id_fuente = ?'
        params.append(source_id)
    
    if platform:
        query += ' AND LOWER(f.tipo_fuente) = LOWER(?)'
        params.append(platform)
    
    query += ' ORDER BY c.fecha_publicacion DESC LIMIT ? OFFSET ?'
    params.extend([limit, offset])
    
    cursor.execute(query, params)
    
    comments = []
    for row in cursor.fetchall():
        comments.append({
            'id': row['id_comentario'],
            'postId': row['id_post'],
            'author': row['autor'],
            'content': row['contenido'],
            'date': row['fecha_publicacion'],
            'likes': row['likes'] or 0,
            'sourceName': row['source_name'],
            'platform': row['platform'],
            'postPreview': row['post_content'][:100] + '...' if row['post_content'] and len(row['post_content']) > 100 else row['post_content'],
            'sentiment': row['sentimiento'],
            'confidence': row['confianza']
        })
    
    conn.close()
    
    return jsonify({
        'comments': comments,
        'total': len(comments),
        'limit': limit,
        'offset': offset
    })

# ============== ESTAD√çSTICAS JER√ÅRQUICAS ==============
@app.route('/api/hierarchy/stats')
def hierarchy_stats():
    """Estad√≠sticas de la estructura jer√°rquica Fuentes‚ÜíPosts‚ÜíComentarios"""
    conn = get_db()
    cursor = conn.cursor()
    
    # Total por nivel
    cursor.execute('SELECT COUNT(*) FROM fuente_osint WHERE activa = 1')
    total_sources = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM dato_recolectado')
    total_posts = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM comentario')
    total_comments = cursor.fetchone()[0]
    
    # Por plataforma
    cursor.execute('''
        SELECT 
            f.tipo_fuente as platform,
            COUNT(DISTINCT f.id_fuente) as sources,
            COUNT(DISTINCT dr.id_dato) as posts,
            COUNT(DISTINCT c.id_comentario) as comments
        FROM fuente_osint f
        LEFT JOIN dato_recolectado dr ON f.id_fuente = dr.id_fuente
        LEFT JOIN comentario c ON dr.id_dato = c.id_post
        GROUP BY f.tipo_fuente
    ''')
    
    by_platform = []
    for row in cursor.fetchall():
        by_platform.append({
            'platform': row['platform'],
            'sources': row['sources'],
            'posts': row['posts'],
            'comments': row['comments']
        })
    
    # Sentimientos de comentarios
    cursor.execute('''
        SELECT 
            ac.sentimiento,
            COUNT(*) as count
        FROM analisis_comentario ac
        GROUP BY ac.sentimiento
    ''')
    
    comments_sentiment = {'Positivo': 0, 'Neutral': 0, 'Negativo': 0}
    for row in cursor.fetchall():
        if row['sentimiento'] in comments_sentiment:
            comments_sentiment[row['sentimiento']] = row['count']
    
    conn.close()
    
    return jsonify({
        'totals': {
            'sources': total_sources,
            'posts': total_posts,
            'comments': total_comments
        },
        'byPlatform': by_platform,
        'commentsSentiment': {
            'positive': comments_sentiment['Positivo'],
            'neutral': comments_sentiment['Neutral'],
            'negative': comments_sentiment['Negativo']
        }
    })

@app.route('/api/hierarchy/tree')
def hierarchy_tree():
    """√Årbol jer√°rquico completo para visualizaci√≥n"""
    conn = get_db()
    cursor = conn.cursor()
    
    # Obtener estructura completa
    cursor.execute('''
        SELECT 
            f.id_fuente,
            f.nombre_fuente,
            f.tipo_fuente,
            dr.id_dato as post_id,
            SUBSTR(dr.contenido_original, 1, 80) as post_preview,
            dr.fecha_publicacion,
            dr.engagement_comments,
            (SELECT COUNT(*) FROM comentario c WHERE c.id_post = dr.id_dato) as collected_comments
        FROM fuente_osint f
        LEFT JOIN dato_recolectado dr ON f.id_fuente = dr.id_fuente
        ORDER BY f.tipo_fuente, f.nombre_fuente, dr.fecha_publicacion DESC
    ''')
    
    # Construir √°rbol
    tree = {}
    for row in cursor.fetchall():
        source_id = row['id_fuente']
        if source_id not in tree:
            tree[source_id] = {
                'id': source_id,
                'name': row['nombre_fuente'],
                'platform': row['tipo_fuente'],
                'posts': []
            }
        
        if row['post_id']:
            tree[source_id]['posts'].append({
                'id': row['post_id'],
                'preview': row['post_preview'] + '...' if row['post_preview'] else '',
                'date': row['fecha_publicacion'],
                'totalComments': row['engagement_comments'] or 0,
                'collectedComments': row['collected_comments'] or 0
            })
    
    conn.close()
    
    return jsonify(list(tree.values()))

# ============== TIKTOK SCRAPING INTERACTIVO (SSE) ==============

from flask import Response, stream_with_context

@app.route('/api/tiktok/scraping/start/<int:source_id>', methods=['POST'])
def start_tiktok_interactive_scraping(source_id):
    """
    Inicia una sesi√≥n de scraping interactivo de TikTok.
    Retorna un session_id para conectarse al stream de eventos.
    """
    try:
        from tiktok_scraping_service import start_tiktok_scraping
        
        session = start_tiktok_scraping(source_id)
        
        return jsonify({
            'success': True,
            'session_id': session.session_id,
            'message': 'Sesi√≥n de scraping iniciada'
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/tiktok/scraping/events/<session_id>')
def tiktok_scraping_events(session_id):
    """
    Stream de eventos SSE para una sesi√≥n de scraping.
    El frontend se conecta aqu√≠ para recibir actualizaciones en tiempo real.
    """
    from tiktok_scraping_service import get_session
    
    session = get_session(session_id)
    if not session:
        return jsonify({'error': 'Sesi√≥n no encontrada'}), 404
    
    def generate():
        for event in session.get_events():
            yield event
    
    return Response(
        stream_with_context(generate()),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'X-Accel-Buffering': 'no'
        }
    )


@app.route('/api/tiktok/scraping/continue/<session_id>', methods=['POST'])
def tiktok_scraping_continue(session_id):
    """
    El usuario confirma que puede continuar (ej: resolvi√≥ CAPTCHA).
    """
    from tiktok_scraping_service import get_session
    
    session = get_session(session_id)
    if not session:
        return jsonify({'error': 'Sesi√≥n no encontrada'}), 404
    
    session.user_continue()
    return jsonify({'success': True, 'message': 'Continuando scraping'})


@app.route('/api/tiktok/scraping/cancel/<session_id>', methods=['POST'])
def tiktok_scraping_cancel(session_id):
    """
    Cancela una sesi√≥n de scraping.
    """
    from tiktok_scraping_service import get_session, cleanup_session
    
    session = get_session(session_id)
    if not session:
        return jsonify({'error': 'Sesi√≥n no encontrada'}), 404
    
    session.cancel()
    cleanup_session(session_id)
    return jsonify({'success': True, 'message': 'Scraping cancelado'})


@app.route('/api/tiktok/scraping/status/<session_id>')
def tiktok_scraping_status(session_id):
    """
    Obtiene el estado actual de una sesi√≥n de scraping.
    """
    from tiktok_scraping_service import get_session
    
    session = get_session(session_id)
    if not session:
        return jsonify({'error': 'Sesi√≥n no encontrada'}), 404
    
    return jsonify({
        'session_id': session_id,
        'running': session.running,
        'waiting_for_user': session.waiting_for_user,
        'cancelled': session.cancelled,
        'stats': session.stats
    })


if __name__ == '__main__':
    print('''
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë       OSINT EMI - API con Datos REALES (SQLite3)         ‚ïë
    ‚ïë                                                          ‚ïë
    ‚ïë  Estructura: Fuentes ‚Üí Posts ‚Üí Comentarios               ‚ïë
    ‚ïë  Puerto: 5001                                            ‚ïë
    ‚ïë  Base de datos: data/osint_emi.db                        ‚ïë
    ‚ïë                                                          ‚ïë
    ‚ïë  ‚ú® TikTok Scraping Interactivo con SSE habilitado       ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    ''')
    # use_reloader=False para evitar problemas con nohup
    app.run(host='0.0.0.0', port=5001, debug=False, use_reloader=False, threaded=True)
